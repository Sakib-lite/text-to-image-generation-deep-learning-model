{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmueDYuRFZQ3rpiABvb6AF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fb74728e5874dbd938567651012bc8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bb165a021e5442da943fe348d1a5e77",
              "IPY_MODEL_25752f8840f342d28e3ef94204b9e969",
              "IPY_MODEL_fb37fef3fa01432ba8cb2a9be4f99451"
            ],
            "layout": "IPY_MODEL_054501a33f8b41209369e3b0baaca399"
          }
        },
        "1bb165a021e5442da943fe348d1a5e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb050604bd7e4fde94b561672f038624",
            "placeholder": "​",
            "style": "IPY_MODEL_c67072eb7e634de0a14eae9bcd410543",
            "value": "config.json: 100%"
          }
        },
        "25752f8840f342d28e3ef94204b9e969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1328b9d4844c48ffba6b332738805743",
            "max": 491,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3f5d7889a0144069c5e2cf574f0a1c2",
            "value": 491
          }
        },
        "fb37fef3fa01432ba8cb2a9be4f99451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2abd92d78c614021bec65028e843b9ea",
            "placeholder": "​",
            "style": "IPY_MODEL_e1c743ed8aed416a8854c9623dc61d66",
            "value": " 491/491 [00:00&lt;00:00, 26.1kB/s]"
          }
        },
        "054501a33f8b41209369e3b0baaca399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb050604bd7e4fde94b561672f038624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c67072eb7e634de0a14eae9bcd410543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1328b9d4844c48ffba6b332738805743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f5d7889a0144069c5e2cf574f0a1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2abd92d78c614021bec65028e843b9ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c743ed8aed416a8854c9623dc61d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8943e357c122414cbe393cd42a2bb6d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db2f17c40e0246688779ad28cede642d",
              "IPY_MODEL_4d1e1ea8ee014253baa1cc4f46236c2f",
              "IPY_MODEL_c12d8448635548b7bfe74b2458296d6e"
            ],
            "layout": "IPY_MODEL_5a4302434ddf44099ae61012d32089a4"
          }
        },
        "db2f17c40e0246688779ad28cede642d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a70950a175d474da6ea51002592fc3e",
            "placeholder": "​",
            "style": "IPY_MODEL_d309de31e93d4abeb4baaa4b8dbe913d",
            "value": "vocab.txt: 100%"
          }
        },
        "4d1e1ea8ee014253baa1cc4f46236c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a76660b4cf049b4afaf9a9c47da19a5",
            "max": 2237676,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d8d068ff05c48139055b156c3c45b1e",
            "value": 2237676
          }
        },
        "c12d8448635548b7bfe74b2458296d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae74571443441d690768d99b83a3a39",
            "placeholder": "​",
            "style": "IPY_MODEL_cd49f73c9d8a43afa145f50c10655ba2",
            "value": " 2.24M/2.24M [00:00&lt;00:00, 17.8MB/s]"
          }
        },
        "5a4302434ddf44099ae61012d32089a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a70950a175d474da6ea51002592fc3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d309de31e93d4abeb4baaa4b8dbe913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a76660b4cf049b4afaf9a9c47da19a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8d068ff05c48139055b156c3c45b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cae74571443441d690768d99b83a3a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd49f73c9d8a43afa145f50c10655ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakib-lite/text-to-image-generation-deep-learning-model/blob/main/text2image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries**"
      ],
      "metadata": {
        "id": "7fCjkUZo0xPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BqV4OdRmwKgo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mouting google drive**\n"
      ],
      "metadata": {
        "id": "0lAQOROs2VOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "496E5i1d2bHu",
        "outputId": "31740a73-180c-4bdb-8230-fc42fdba8eb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting the dataset zip file**"
      ],
      "metadata": {
        "id": "ayqyifuw4-VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/Colab_Datasets/BNATURE.zip'\n",
        "extract_to = '/content/bnature'\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Extraction complete. Contents are now in:\", extract_to)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW5CMODa4q5v",
        "outputId": "a2447c4f-4fda-45c9-a9e3-5c74856c3fdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete. Contents are now in: /content/bnature\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Picure and Caption file path**"
      ],
      "metadata": {
        "id": "FwR7W8dc2HgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder = '/content/bnature/'\n",
        "image_folder = os.path.join(dataset_folder, 'Pictures')\n",
        "caption_file = os.path.join(dataset_folder, 'caption/caption.txt')"
      ],
      "metadata": {
        "id": "82dMTfOX2NiF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking total images**"
      ],
      "metadata": {
        "id": "pzNC5-Ap5nYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "image_count = len(image_files)\n",
        "\n",
        "print(f\"Total images: {image_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKfT3e9e2vlK",
        "outputId": "32625401-2a74-4d04-cf33-1775564edfa9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wY9fwDZK24bt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Captions**"
      ],
      "metadata": {
        "id": "zcjWcN8U9oJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions(caption_file):\n",
        "    caption_mapping = {}\n",
        "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('.jpg', 1)\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            image_file = parts[0].strip() + '.jpg'\n",
        "            caption = parts[1].strip()\n",
        "            if image_file not in caption_mapping:\n",
        "                caption_mapping[image_file] = []\n",
        "            caption_mapping[image_file].append(caption)\n",
        "    return caption_mapping\n",
        "\n",
        "captions = load_captions(caption_file)\n",
        "print(\"Sample captions:\", list(captions.items())[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE_hSF_F6Q7O",
        "outputId": "dcfeab53-074a-4f09-a8b2-b887b5efad9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample captions: [('1.jpg', ['গ্রামে হাঁটা দুই শিশু।', 'দুই শিশু গ্রামে হাঁটছে।', 'একটা নীল জামা পড়া শিশু।', 'দুই শিশু খেলা করে।', 'দুইটা শিশু দাঁড়িয়ে আছে।']), ('2.jpg', ['কিছু লোক নদী পার করছে।', 'কিছু লোক বিল পার করছে।', 'অনেকগুলো লোক নদী পার করছে।', 'জমির ভিতর অনেকগুলো লোক।', 'কিছু লোক নদী পার করছে।']), ('3.jpg', ['এক কৃষক ক্ষেতে কাজ করছে।', 'এক কৃষক ধান ক্ষেতে কাজ করছে।', 'রোদ এর নিচে একটা লোক দাঁড়িয়ে আছে।', 'এক কৃষক ধান দেখছে।', 'শার্ট পড়া একটা লোক।'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEPS**\n",
        "\n",
        "\n",
        "1.   ***Import necessary libraries***\n",
        "2.   ***Mount google drive***\n",
        "3.   ***Extract ZIP***\n",
        "4.   ***Load Captions***\n",
        "4.   ***Pre Process images(Resize)***\n",
        "4.   ***Tokenize caption with bangla-bert***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PgclQkav53jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre process image function**"
      ],
      "metadata": {
        "id": "galWrrkDDEMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path, target_size=(128, 128)):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "    return img_array\n",
        "\n",
        "\n",
        "# TESTING\n",
        "sample_image_path = os.path.join(image_folder, '1.jpg')\n",
        "sample_image = preprocess_image(sample_image_path)\n",
        "print(\"Sample image shape:\", sample_image.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd_dqu6M8kdc",
        "outputId": "e39e45d8-5d4b-44f7-ad0f-19ef607ef8dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image shape: (128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize caption with bangla-bert**"
      ],
      "metadata": {
        "id": "M8Lx2j0jDRQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
        "\n",
        "def tokenize_caption(caption):\n",
        "    tokenized = tokenizer(caption, padding=\"max_length\", truncation=True, max_length=20, return_tensors=\"tf\")\n",
        "    input_ids = tf.squeeze(tokenized[\"input_ids\"], axis=0)\n",
        "    attention_mask = tf.squeeze(tokenized[\"attention_mask\"], axis=0)\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "sample_caption = captions['1.jpg'][0]\n",
        "input_ids, attention_mask = tokenize_caption(sample_caption)\n",
        "print(\"Tokenized caption:\", input_ids.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "1fb74728e5874dbd938567651012bc8e",
            "1bb165a021e5442da943fe348d1a5e77",
            "25752f8840f342d28e3ef94204b9e969",
            "fb37fef3fa01432ba8cb2a9be4f99451",
            "054501a33f8b41209369e3b0baaca399",
            "cb050604bd7e4fde94b561672f038624",
            "c67072eb7e634de0a14eae9bcd410543",
            "1328b9d4844c48ffba6b332738805743",
            "e3f5d7889a0144069c5e2cf574f0a1c2",
            "2abd92d78c614021bec65028e843b9ea",
            "e1c743ed8aed416a8854c9623dc61d66",
            "8943e357c122414cbe393cd42a2bb6d9",
            "db2f17c40e0246688779ad28cede642d",
            "4d1e1ea8ee014253baa1cc4f46236c2f",
            "c12d8448635548b7bfe74b2458296d6e",
            "5a4302434ddf44099ae61012d32089a4",
            "2a70950a175d474da6ea51002592fc3e",
            "d309de31e93d4abeb4baaa4b8dbe913d",
            "5a76660b4cf049b4afaf9a9c47da19a5",
            "5d8d068ff05c48139055b156c3c45b1e",
            "cae74571443441d690768d99b83a3a39",
            "cd49f73c9d8a43afa145f50c10655ba2"
          ]
        },
        "id": "CQFLOh-K-DlH",
        "outputId": "b111d242-dd98-4849-c4f7-b27d34afa162"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fb74728e5874dbd938567651012bc8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8943e357c122414cbe393cd42a2bb6d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized caption: [  101 32993 33532 70919  7932 72990  1014   102     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataset**"
      ],
      "metadata": {
        "id": "Z0JmFtJ9GM4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(image_folder, captions, batch_size=8, target_size=(128, 128)):\n",
        "    image_paths = []\n",
        "    all_captions = []\n",
        "\n",
        "    for image_file, caption_list in captions.items():\n",
        "        for caption in caption_list:\n",
        "            image_paths.append(os.path.join(image_folder, image_file))\n",
        "            all_captions.append(caption)\n",
        "\n",
        "    def generator():\n",
        "        for img_path, caption in zip(image_paths, all_captions):\n",
        "            img_array = preprocess_image(img_path, target_size)\n",
        "            input_ids, attention_mask = tokenize_caption(caption)\n",
        "            yield img_array, (input_ids, attention_mask)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(target_size[0], target_size[1], 3), dtype=tf.float32),\n",
        "            (\n",
        "                tf.TensorSpec(shape=(20,), dtype=tf.int32),\n",
        "                tf.TensorSpec(shape=(20,), dtype=tf.int32),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=len(image_paths)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "dataset = create_dataset(image_folder, captions, batch_size=4)"
      ],
      "metadata": {
        "id": "erGL9fnuCQ0S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_images, (batch_input_ids, batch_attention_masks) in dataset.take(1):\n",
        "    print(\"Batch images shape:\", batch_images.shape)\n",
        "    print(\"Batch input IDs shape:\", batch_input_ids.shape)\n",
        "    print(\"Batch attention masks shape:\", batch_attention_masks.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SP7h8W4DLNv",
        "outputId": "1d9e8e87-e3bb-4eae-9258-c53728eb6c54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images shape: (4, 128, 128, 3)\n",
            "Batch input IDs shape: (4, 20)\n",
            "Batch attention masks shape: (4, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n"
      ],
      "metadata": {
        "id": "OcRzSz_vGUNe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "> The ConditioningAugmentation class is a custom TensorFlow/Keras layer designed to process text embeddings for further use in tasks like image generation. It applies a series of transformations to the input text embeddings, including dimensionality reduction and the addition of noise, to make the embeddings more robust and suitable for downstream tasks.\n",
        "\n",
        "**embedding_dim**: The dimensionality of the input text embedding.\n",
        "\n",
        "**latent_dim**: The target dimensionality of the processed embedding (often smaller than embedding_dim).\n",
        "\n",
        "**self.dense**: A fully connected layer (Dense) that maps the high-dimensional text embedding (embedding_dim) to a lower-dimensional latent space (latent_dim).\n",
        "\n",
        "**self.activation**: A LeakyReLU activation function to introduce non-linearity and handle negative values in a more nuanced way than regular ReLU.\n",
        "\n",
        "**self.noise**: A utility for generating random noise (Gaussian noise in this case).\n",
        "\n"
      ],
      "metadata": {
        "id": "s-4u9tCwqtv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditioningAugmentation(layers.Layer):\n",
        "    def __init__(self, embedding_dim, latent_dim):\n",
        "        super(ConditioningAugmentation, self).__init__()\n",
        "        self.dense = layers.Dense(latent_dim)\n",
        "        self.activation = layers.LeakyReLU()\n",
        "        self.noise = tf.random.normal\n",
        "\n",
        "    def call(self, text_embedding):\n",
        "        projected = self.dense(text_embedding)\n",
        "        conditioned = self.activation(projected)\n",
        "        noise = self.noise(tf.shape(conditioned))\n",
        "        return conditioned + noise\n"
      ],
      "metadata": {
        "id": "qMiF4EcWqgql"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The upsampling_block function defines a block for upsampling in a deep learning model, typically used in image generation tasks or semantic segmentation. Upsampling increases the spatial dimensions (width and height) of the feature map while applying transformations to extract meaningful features."
      ],
      "metadata": {
        "id": "Vfq3yDsmu5ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsampling_block(x, filters, kernel_size=4, strides=2, padding=\"same\"):\n",
        "    x = layers.Conv2DTranspose(filters, kernel_size, strides, padding)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "UH3u4Ptmug9z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This function build_generator defines the generator model for a GAN (Generative Adversarial Network). The generator's goal is to create synthetic images based on the provided text embeddings and noise, progressively upsampling a low-resolution representation into a high-resolution image."
      ],
      "metadata": {
        "id": "ufa8Z0_SvJ1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(text_embedding_dim, noise_dim, image_size=(128, 128, 3)):\n",
        "    text_input = layers.Input(shape=(text_embedding_dim,))\n",
        "    noise_input = layers.Input(shape=(noise_dim,))\n",
        "\n",
        "    # Conditioning augmentation\n",
        "    conditioning = ConditioningAugmentation(text_embedding_dim, 128)\n",
        "    conditioned_text = conditioning(text_input)\n",
        "\n",
        "    # Combine text embeddings and noise\n",
        "    combined_input = layers.Concatenate()([conditioned_text, noise_input])\n",
        "\n",
        "    # Initial dense layer to form a low-resolution image\n",
        "    x = layers.Dense(4 * 4 * 512, activation=\"relu\")(combined_input)\n",
        "    x = layers.Reshape((4, 4, 512))(x)\n",
        "\n",
        "    # Upsampling to the target image size\n",
        "    x = upsampling_block(x, 256)  # 8x8\n",
        "    x = upsampling_block(x, 128)  # 16x16\n",
        "    x = upsampling_block(x, 64)   # 32x32\n",
        "    x = upsampling_block(x, 32)   # 64x64\n",
        "    x = upsampling_block(x, 16)   # 128x128\n",
        "\n",
        "    # Output layer\n",
        "    output = layers.Conv2DTranspose(image_size[-1], kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\")(x)\n",
        "\n",
        "    return Model(inputs=[text_input, noise_input], outputs=output, name=\"Generator\")\n"
      ],
      "metadata": {
        "id": "Gpl1vMDnu_gb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the Generator**"
      ],
      "metadata": {
        "id": "NqC74AFlvhiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "text_embedding_dim = 768  # Output dimension of Bangla BERT\n",
        "noise_dim = 100  # Dimension of random noise vector\n",
        "image_size = (128, 128, 3)\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(text_embedding_dim, noise_dim, image_size)\n",
        "\n",
        "# Test the generator\n",
        "sample_text_embedding = tf.random.normal((8, text_embedding_dim))  # Simulate 8 text embeddings\n",
        "sample_noise = tf.random.normal((8, noise_dim))  # Simulate 8 noise vectors\n",
        "\n",
        "# Generate images\n",
        "generated_images = generator([sample_text_embedding, sample_noise])\n",
        "print(\"Generated images shape:\", generated_images.shape)  # Expected: (8, 128, 128, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sxiYZ-HvUyG",
        "outputId": "2ed53eb9-bc56-45be-f0cb-772a4bf76288"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated images shape: (8, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discriminator Implementation**\n"
      ],
      "metadata": {
        "id": "qqr4NzxBvwo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(image_size=(128, 128, 3), text_embedding_dim=768):\n",
        "    # Image input\n",
        "    image_input = layers.Input(shape=image_size)\n",
        "\n",
        "    # Text embedding input\n",
        "    text_input = layers.Input(shape=(text_embedding_dim,))\n",
        "\n",
        "    # Image feature extractor\n",
        "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(image_input)  # 64x64\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)  # 32x32\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\")(x)  # 16x16\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\")(x)  # 8x8\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Concatenate image features with text embeddings\n",
        "    combined_features = layers.Concatenate()([x, text_input])\n",
        "    combined_features = layers.Dense(512, activation=\"relu\")(combined_features)\n",
        "\n",
        "    # Adversarial head: Real vs. Fake classification\n",
        "    adv_output = layers.Dense(1, activation=\"sigmoid\", name=\"Adversarial\")(combined_features)\n",
        "\n",
        "    # Matching head: Text-image alignment score\n",
        "    match_output = layers.Dense(1, activation=\"sigmoid\", name=\"Matching\")(combined_features)\n",
        "\n",
        "    # Build model\n",
        "    discriminator = Model(inputs=[image_input, text_input], outputs=[adv_output, match_output], name=\"Discriminator\")\n",
        "    return discriminator\n"
      ],
      "metadata": {
        "id": "65_BqsmZvmxl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the Discriminator**\n",
        "\n"
      ],
      "metadata": {
        "id": "8NI4ANajv3G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (128, 128, 3)\n",
        "text_embedding_dim = 768\n",
        "\n",
        "discriminator = build_discriminator(image_size, text_embedding_dim)\n",
        "\n",
        "# Test the discriminator\n",
        "sample_images = tf.random.normal((8, *image_size))  # Simulate 8 images\n",
        "sample_text_embeddings = tf.random.normal((8, text_embedding_dim))  # Simulate 8 text embeddings\n",
        "\n",
        "# Get predictions\n",
        "adv_preds, match_preds = discriminator([sample_images, sample_text_embeddings])\n",
        "print(\"Adversarial predictions shape:\", adv_preds.shape)  # Expected: (8, 1)\n",
        "print(\"Matching predictions shape:\", match_preds.shape)    # Expected: (8, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNZQLqcIv0-u",
        "outputId": "73dab045-7a4d-403f-c7fd-016d4d37ca3c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial predictions shape: (8, 1)\n",
            "Matching predictions shape: (8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement Loss Functions**"
      ],
      "metadata": {
        "id": "xbhVTtFVwqm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_loss(real_preds, fake_preds):\n",
        "    # Binary cross-entropy loss\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    real_loss = bce(tf.ones_like(real_preds), real_preds)  # Real images classified as real\n",
        "    fake_loss = bce(tf.zeros_like(fake_preds), fake_preds)  # Fake images classified as fake\n",
        "    return real_loss + fake_loss\n"
      ],
      "metadata": {
        "id": "MrWkCtzXwAbZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generator Loss**"
      ],
      "metadata": {
        "id": "YNCUDGWNw0nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_preds):\n",
        "    # Generator tries to make fake images look real\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    return bce(tf.ones_like(fake_preds), fake_preds)\n"
      ],
      "metadata": {
        "id": "01DuYWxHwvdM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matching Loss**"
      ],
      "metadata": {
        "id": "LO9tiYUhw9GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_loss(real_text_preds, fake_text_preds):\n",
        "    # Match generated image with corresponding text\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    real_loss = bce(tf.ones_like(real_text_preds), real_text_preds)\n",
        "    fake_loss = bce(tf.zeros_like(fake_text_preds), fake_text_preds)\n",
        "    return real_loss + fake_loss\n"
      ],
      "metadata": {
        "id": "yKNhMf1Zw5mx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Loss Functions**"
      ],
      "metadata": {
        "id": "Jvtb7sOXxIB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate predictions\n",
        "real_preds = tf.random.uniform((8, 1), 0.7, 1.0)  # Discriminator's output for real images\n",
        "fake_preds = tf.random.uniform((8, 1), 0.0, 0.3)  # Discriminator's output for fake images\n",
        "real_text_preds = tf.random.uniform((8, 1), 0.7, 1.0)  # Matching head output for real text-image pair\n",
        "fake_text_preds = tf.random.uniform((8, 1), 0.0, 0.3)  # Matching head output for fake text-image pair\n",
        "\n",
        "# Calculate losses\n",
        "disc_loss = adversarial_loss(real_preds, fake_preds)\n",
        "gen_loss = generator_loss(fake_preds)\n",
        "match_loss = matching_loss(real_text_preds, fake_text_preds)\n",
        "\n",
        "print(\"Discriminator Loss:\", disc_loss.numpy())\n",
        "print(\"Generator Loss:\", gen_loss.numpy())\n",
        "print(\"Matching Loss:\", match_loss.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUXildACxENR",
        "outputId": "7e412b39-c082-4f14-cce1-4a7c5dbba8de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator Loss: 0.38897952\n",
            "Generator Loss: 2.3805423\n",
            "Matching Loss: 0.38053903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Optimizers**"
      ],
      "metadata": {
        "id": "_A5dkpSQxjDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n"
      ],
      "metadata": {
        "id": "DfDxGxdyxOoB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Step Function**"
      ],
      "metadata": {
        "id": "EpG_bpqzxwYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images, text_embeddings, attention_masks, generator, discriminator):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    noise_dim = 100\n",
        "    random_noise = tf.random.normal((batch_size, noise_dim))\n",
        "\n",
        "    # Train Discriminator\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        # Real images\n",
        "        real_preds, real_match_preds = discriminator([images, text_embeddings], training=True)\n",
        "\n",
        "        # Fake images\n",
        "        generated_images = generator([text_embeddings, random_noise], training=True)\n",
        "        fake_preds, fake_match_preds = discriminator([generated_images, text_embeddings], training=True)\n",
        "\n",
        "        # Discriminator Loss\n",
        "        d_loss = adversarial_loss(real_preds, fake_preds) + matching_loss(real_match_preds, fake_match_preds)\n",
        "\n",
        "    disc_gradients = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    # Train Generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        # Generate fake images\n",
        "        generated_images = generator([text_embeddings, random_noise], training=True)\n",
        "        fake_preds, fake_match_preds = discriminator([generated_images, text_embeddings], training=True)\n",
        "\n",
        "        # Generator Loss\n",
        "        g_loss = generator_loss(fake_preds) + matching_loss(real_match_preds, fake_match_preds)\n",
        "\n",
        "    gen_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "\n",
        "    return d_loss, g_loss\n"
      ],
      "metadata": {
        "id": "oHj7sMXOxsvx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full Training Loop**"
      ],
      "metadata": {
        "id": "U0UPrhvZx3XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, generator, discriminator, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for step, (images, (text_embeddings, attention_masks)) in enumerate(dataset):\n",
        "            d_loss, g_loss = train_step(images, text_embeddings, attention_masks, generator, discriminator)\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: Discriminator Loss = {d_loss.numpy()}, Generator Loss = {g_loss.numpy()}\")\n",
        "\n",
        "        # Save model and generate sample images\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            generator.save(f\"generator_epoch_{epoch + 1}.h5\")\n",
        "            discriminator.save(f\"discriminator_epoch_{epoch + 1}.h5\")\n",
        "\n",
        "            # Generate and save images for inspection\n",
        "            random_noise = tf.random.normal((8, noise_dim))\n",
        "            generated_images = generator([text_embeddings[:8], random_noise])\n",
        "            save_generated_images(generated_images, epoch + 1)\n"
      ],
      "metadata": {
        "id": "Nox1MQKMx0d5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Sample Images**"
      ],
      "metadata": {
        "id": "ifPljNikyARe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def save_generated_images(images, epoch):\n",
        "    output_dir = \"generated_images\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    images = (images + 1) / 2.0  # Rescale to [0, 1]\n",
        "    for i, img in enumerate(images):\n",
        "        plt.imsave(os.path.join(output_dir, f\"epoch_{epoch}_img_{i}.png\"), img.numpy())\n"
      ],
      "metadata": {
        "id": "NiAg3xwpx77C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Train the Model**"
      ],
      "metadata": {
        "id": "nGflPk7b1VCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [ ] while(1): a.append('1')\n",
        "\n",
        "generator_lr = 0.0002\n",
        "discriminator_lr = 0.0002\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(generator_lr, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(discriminator_lr, beta_1=0.5)\n",
        "\n",
        "# Step 2: Define Training Hyperparameters\n",
        "epochs = 20\n",
        "noise_dim = 100\n",
        "\n",
        "train(dataset, generator, discriminator, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT0NxPEE1VyD",
        "outputId": "8b4e0044-f13c-4d0b-dbed-0baf9b572ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERp_Mpsp2CkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}